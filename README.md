# Spotify-Realtime-Data-Pipeline

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-3.4-black.svg)](https://kafka.apache.org/)
[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.9-017CEE.svg)](https://airflow.apache.org/)
[![Snowflake](https://img.shields.io/badge/Snowflake-Cloud%20DWH-29B5E8.svg)](https://www.snowflake.com/)
[![dbt](https://img.shields.io/badge/dbt-1.5+-FF694B.svg)](https://www.getdbt.com/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED.svg)](https://www.docker.com/)

## ğŸ“Œ Project Overview

This project implements a **production-grade, end-to-end real-time Spotify Data Pipeline** using a **modern data stack**. It simulates high-volume streaming music events and processes them through scalable ingestion, transformation, and analytics layers.

The pipeline is fully automated and designed to mirror **real-world data platforms**.

---

## ğŸ—ï¸ Technical Architecture

<img width="5600" height="2898" alt="Architectur" src="https://github.com/kunalvijay42/Spotify-Realtime-Data-Pipeline/blob/main/Spotify%20Real%20Time%20Pipeline%20Architecture%20Diagram.png" />

---

## ğŸš€ Data Pipeline Flow

The system implements a **medallion architecture** with the following components:

1. **Data Generation Layer**  
   Synthetic Spotify streaming events (user activity, track metadata, geographic location, device information) generated via Python Faker library

2. **Streaming Layer**  
   Real-time event streaming through Apache Kafka topics with configurable throughput and partitioning

3. **Ingestion Layer**  
   Kafka consumers persist raw events to MinIO object storage (S3-compatible) for reliable data lake storage

4. **Orchestration Layer**  
   Apache Airflow DAGs automate incremental data loading from MinIO to Snowflake Bronze layer on scheduled intervals

5. **Data Warehouse Layer**  
   Snowflake manages data across three layers following medallion architecture:
   - **Bronze:** Raw, unprocessed streaming data
   - **Silver:** Cleaned, validated, and conformed data
   - **Gold:** Business-ready aggregated analytics tables

6. **Transformation Layer**  
   dbt (data build tool) performs SQL-based transformations and data quality testing directly within Snowflake

---

## ğŸ‘©â€ğŸ’» Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Data Generation** | Python (Faker) | Synthetic streaming event generation |
| **Message Streaming** | Apache Kafka | Real-time event streaming platform |
| **Object Storage** | MinIO | S3-compatible data lake storage |
| **Data Warehouse** | Snowflake | Cloud-native data warehouse |
| **Transformation** | dbt (data build tool) | SQL transformations, testing, documentation |
| **Orchestration** | Apache Airflow | Workflow automation and scheduling |
| **Containerization** | Docker & Docker Compose | Reproducible infrastructure deployment |

---

## ğŸ“‚ Github Repository Structure

```text
spotify-realtime-data-pipeline/
â”œâ”€â”€ docker/ # Airflow DAGs for orchestration
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ minio-to-kafka.py
â”‚       â””â”€â”€ .env
â”œâ”€â”€ dbt/  # DBT Files
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ gold/
â”‚       â”œâ”€â”€ silver/
â”‚       â””â”€â”€ sources.yml
â”œâ”€â”€ simulator/ # Python Simulator Producer
â”‚   â”œâ”€â”€ producer.py
â”‚   â””â”€â”€ .env
â”œâ”€â”€ consumer/ # Kafka Consumer 
â”‚   â”œâ”€â”€ kafka-to-minio.py
â”‚   â””â”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---
## ğŸš€ Steps to run the Project

```bash
# Clone the repository
git clone https://github.com/kunalvijay42/Spotify-Realtime-Data-Pipeline.git

# Navigate to project directory
cd Spotify-Realtime-Data-Pipeline

# Start all services
docker-compose up -d
```
---